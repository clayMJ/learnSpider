Scrapy爬虫框架结构:半成品
5+2结构
（ENGINE SCHEDULER） ITEM PIPELINES （DOWNLOADER） SPIDERS
()都为已编写好,不需要编写

“Engine
控制所有模块之间数据流
根据条件触发事件

Downloader
根据请求下载页面

Scheduler
对所有爬去请求进行调度管理"

Downloader Middleware:
目的：实现""之间进行用户可配置的控制
功能：修改丢弃新增请求或响应

Spider(核心)
解析Downloader返回的响应(Response)
产生爬取项(scraped item)
产生额外的爬取请求(Request)

Item Piplines
以流水线方式处理Spider产生爬取项
每个操作时一个Item Pipeline类型
包含操作:清理、检验查查等再放入数据库

Spider Middleware
目的：对请求和爬取项对在处理
功能：修改、丢弃、新增请求或爬取项

Requests   vs   Scrapy
网页级          网站级
功能库          框架
性能较差        性能较高
重点页面下载    重点在与Spider
定制灵活        深度定制较难
上手简单        入门稍难


startproject
genspider
settings
crawl
list            列出所有spider
shell           启动URL调试命令行


python123demo/             外层目录
    scrapy.cfg             部署scrapy爬虫配置文件
	python123demo/         scrapy框架用户自定义python代码
	    __init__.py        初始化脚本
		items.py           Items代码模版(继承类)
	    middlewares.py     Middlewares代码模版(继承类)
        pipelines.py       pipelines代码模版(继承类)
		settings.py        scrapy爬虫配置文件
        spiders/           spiders代码模版目录(继承类)


Scrapy步骤
建立一个Scrapy爬虫工程
在工程中产生一个Scrapy爬虫
配置spider爬虫
运行爬虫，获取网页


yield关键词
       yield   生成器
优势：
更节省存储空间  响应更迅速  使用更灵活
eg:

def gen(n):
	for i in range(n):
		yield i**2
	

for i in gen(5):
	print(i, "", end="")
##结果为0 1 4 9 16



Request类
.url
.method          对应请求方法 'GET'  'POST'
.headers         字典类型风格请求头
.body            字符串类型，请求内容主体
.meta            用户添加的拓展星系，在scrapy内部模块间传递信息使用 
.copy()          复制该请求



Response类
class scrapy.http.Response()
response表示一个http响应
由downloader生成 由spider处理

.url
.status          默认200  
.headers
.body
.flags
.request
.copy()



Item类
class scrapy.item.Item()
Item对象表示一个从html页面提取的信息内容
由Iitme Pipeline处理
Item类字典类型


Scrapy提取html信息方法
BeautifulSoup
lxml
re
XPath Selector
CSS Selector

CSS Selector
<HTML>.css('a::attr(href)').extract()



